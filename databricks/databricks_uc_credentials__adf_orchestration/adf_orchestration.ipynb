{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd22ca15-815b-442b-b871-095aabfa0c91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-mgmt-datafactory azure-identity\n",
    "\n",
    "import os\n",
    "import time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.mgmt.datafactory import DataFactoryManagementClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688cb7ba-f2a9-4a6f-8249-9cb9d45100ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# set env variable\n",
    "ENV_NAME = os.getenv('ENV_NAME','')\n",
    "spark.sql('SET var.ENV_NAME = '+ ENV_NAME)\n",
    "\n",
    "SUBSCRIPTION_NAME = dbutils.secrets.get(scope=f\"dataplatform{ENV_NAME}-dbss\", key=\"subscription-id\")\n",
    "RESOURCE_GROUP_NAME = f\"rg-dataplatform-{ENV_NAME}\"\n",
    "DATA_FACTORY_NAME = f\"adf-{ENV_NAME}\"\n",
    "UC_CREDENTIAL_NAME = f\"dataplatform{ENV_NAME}_azservices-dbsc\"\n",
    "\n",
    "# Recieve ADF pipeline name passed in through parameter\n",
    "PIPELINE_NAME = dbutils.widgets.get(\"p_pipeline_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a55b675-453e-410c-ae6b-db356bcfeb9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "credential=dbutils.credentials.getServiceCredentialsProvider(UC_CREDENTIAL_NAME)\n",
    "\n",
    "adf_client = DataFactoryManagementClient(credential=credential, subscription_id=SUBSCRIPTION_NAME)\n",
    "\n",
    "run_response = adf_client.pipelines.create_run(\n",
    "    resource_group_name=RESOURCE_GROUP_NAME,\n",
    "    factory_name=DATA_FACTORY_NAME,\n",
    "    pipeline_name=PIPELINE_NAME\n",
    ")\n",
    "\n",
    "# Poll status\n",
    "run_id = run_response.run_id\n",
    "print(f\"Pipeline run initiated: {run_id}\")\n",
    "\n",
    "while True:\n",
    "    pipeline_run = adf_client.pipeline_runs.get(\n",
    "        RESOURCE_GROUP_NAME,\n",
    "        DATA_FACTORY_NAME,\n",
    "        run_id\n",
    "    )\n",
    "    status = pipeline_run.status\n",
    "    print(f\"ADF pipeline status: {status}\")\n",
    "    \n",
    "    if status in (\"Succeeded\", \"Failed\", \"Cancelled\"):\n",
    "        break\n",
    "    time.sleep(10)\n",
    "\n",
    "if status != \"Succeeded\":\n",
    "    raise Exception(f\"ADF pipeline failed with status: {status}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ADF Orchestration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
